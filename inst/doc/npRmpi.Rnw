%% $Id: entropy_np.Rnw,v 1.43 2010/02/18 14:43:37 jracine Exp jracine $

%\VignetteIndexEntry{Parallel np: Using the npRmpi Package}
%\VignetteDepends{npRmpi,boot,cubature,MASS}
%\VignetteKeywords{nonparametric, kernel, entropy, econometrics, qualitative,
%categorical}
%\VignettePackage{npRmpi}

\documentclass[nojss]{jss}

%% need no \usepackage{Sweave.sty}

\usepackage{amsmath,amsfonts}
\usepackage[utf8x]{inputenc} 

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\N}{\field{N}}
\newcommand{\bbR}{\field{R}} %% Blackboard R
\newcommand{\bbS}{\field{S}} %% Blackboard S

\author{Jeffrey S.~Racine\\McMaster University}

\title{Parallel np: Using the npRmpi Package}

\Plainauthor{Jeffrey S.~Racine}

\Plaintitle{Parallel np: Using the npRmpi Package}

\Abstract{ 
  
  The \pkg{npRmpi} package is a parallel implementation of the
  \proglang{R} (\citet{R}) package \pkg{np} (\citet{np}). The
  underlying \proglang{C} code uses the message passing interface
  (`\proglang{MPI}') and is \proglang{MPI2} compliant.

}

\Keywords{nonparametric, semiparametric, kernel smoothing,
  categorical data}

\Plainkeywords{Nonparametric, kernel, econometrics, qualitative,
  categorical}

\Address{Jeffrey S.~Racine\\
  Department of Economics\\
  McMaster University\\
  Hamilton, Ontario, Canada, L8S 4L8\\
  E-mail: \email{racinej@mcmaster.ca}\\
  URL: \url{http://www.mcmaster.ca/economics/racine/}\\
}

\begin{document}

%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

%% Note - fragile using \label{} in \section{} - must be outside

%% For graphics

\setkeys{Gin}{width=\textwidth}

%% Achim's request for R prompt set invisibly at the beginning of the
%% paper

\section{Overview}

A common and understandable complaint often voiced about applied
nonparametric kernel methods is the amount of computation time
required for data-driven bandwidth selection when one has a large data
set. There is a certain irony at play here since nonparametric methods
are ideally suited to situations involving large data sets yet,
computationally speaking, their analysis may lie beyond the reach of
many users.  Some background may be in order. My co-authors and I
favor data-driven methods of bandwidth selection such as
cross-validation, among others. These methods possess a number of very
desirable properties but have run times that are proportional to the
square of the number of observations hence doubling the number of
observations will increase run time by a factor of four. For large
data sets run time many simply not be feasible in a serial
(i.e.~single processor) environment.

The solution adopted in the \pkg{npRmpi} package is to run the code in
a parallel computing environment and exploit the presence of multiple
processors when available. The underlying \proglang{C} code for \pkg{np}
is \proglang{MPI}-aware (\proglang{MPI} denotes the `message passing
interface', a popular parallel programming library that is an
international standard), and we merge the \pkg{R np} and \pkg{Rmpi}
packages to form the \pkg{npRmpi} package (this requires minor
modification of some of the underlying \pkg{Rmpi} code which is why we
cannot simply load the \pkg{Rmpi} package itself).\footnote{The
  \pkg{npRmpi} package incorporates the \pkg{Rmpi} package (Hao Yu
  <hyu@stats.uwo.ca>) with minor modifications and we are extremely
  grateful to Hao Yu for his contributions to the \proglang{R}
  community.}

All of the functions in \pkg{np} can exploit the presence of multiple
processors. Run time is inversely proportional to the number of
processors hence doubling the number of processors will cut run time
in half.\footnote{There is minor overhead involved with message
  passing, and for small samples the overhead can be substantial when
  the ratio of message passing to computing the kernel estimator
  increases - this will be negligible for sufficiently large samples.}
Given the availability of commodity cluster computers and the presence
of multiple cores in desktop and laptop machines, leveraging the
\pkg{npRmpi} package for large data sets may present a feasible
solution to the often lengthy computation times associated with
nonparametric kernel methods.

The code has been tested in the Mac OS X and Linux environments which
allow the user to compile \proglang{R} packages on the fly (presuming
of course that a C compiler exists on your system). Users running MS
Windows will have to consult local tech support personnel and may also
wish to consult the resources available for the \pkg{Rmpi} package and
associated web site for further assistance. I cannot assist with
installation issues beyond what is provided in this document and trust
the reader will forgive me for this.

\section{Differences Between np and npRmpi}

There are only a few visible differences between running code in
serial versus parallel environments.  Typically you run your parallel
code in batch mode so the first step would be to get your code running
in batch mode using the \pkg{np} package (obviously on a subset of
your data for large data sets). Once you have properly functioning
code, you will next add some `hooks' necessary for \proglang{MPI} to
run (see Section \ref{sec example} below for a detailed example), and
finally you will run the job using either \code{mpirun} or,
indirectly, via a batch scheduler on your cluster such as \code{sqsub}
(kindly consult your local support personnel for assistance on using
batch queueing systems on your local cluster).

Note that, since the data has to be broadcast to the slave notes, it
is a good idea to put it in a dataframe first and it is always a good
idea at this stage to cast your variables according to type.

\subsection*{Installation}

Installation will depend on your hardware and software configuration
and will vary widely across platforms. If you are not familiar with
parallel computing you are strongly advised to seek local advice.

That being said, if you have current versions of \proglang{R} and Open
\proglang{MPI} properly installed on your system, installation of the
\pkg{npRmpi} package could be as simple as downloading the
\pkg{npRmpi} tarball and, from a command shell, running

\begin{verbatim}
R CMD INSTALL npRmpi_foo.tar.gz 
\end{verbatim}

where foo is the version number. 

For clusters you may additionally need to provide locations of
libraries (kindly see your local sysadmin as there are far too many
variations for me to assist). On a local Linux cluster I use the
following by way of illustration (for this illustration we use the
Intel compiler suite and need to set \proglang{MPI} library paths and
\proglang{MPI} root directories):

\begin{verbatim}
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/sharcnet/openmpi/1.4.2/intel/lib
export MPI_ROOT=/opt/sharcnet/openmpi/1.4.2/intel 
R CMD INSTALL npRmpi_foo.tar.gz
\end{verbatim}

where again foo is the version number. 

Please seek local help for further assistance on installing and
running parallel programs.

\subsection{Parallel Batch Execution}

To run a parallel \pkg{np} job having successfully installed the
\pkg{npRmpi} program, copy the \code{Rprofile} file in
\code{npRmpi/inst} to the current directory and name it
\code{.Rprofile} (or copy it to your home directory and again name it
\code{.Rprofile}).\footnote{You will need to download the \pkg{npRmpi}
  source code and unpack it in order to get Rprofile from the
  npRmpi/inst directory.} Then to run the batch code in the file
\code{npudensml_npRmpi.R} using two processors on an Open \proglang{MPI}
system you will enter something like

\begin{verbatim}
mpirun -np 2 R CMD BATCH npudensml_npRmpi.R
\end{verbatim}

You can compare run times and any other differences by examining the
files \code{npudensml_serial.Rout} and \code{npudensml_npRmpi.Rout}
(see Section \ref{sec run time} below for some illustrative
examples). Clearly you could do this with a subset of your data for
large problems to judge the extent to which the parallel code reduces
run time.

If you have a batch scheduler installed on your cluster you might
instead enter something like

\begin{verbatim}
sqsub -q mpi -n 2 R CMD BATCH npudensml_npRmpi.R
\end{verbatim}

Again, kindly consult local tech support personnel for issues
concerning the use of batch queueing systems and using compute
clusters.

\subsection{Essential Program Elements}\label{sec example}

Here is a simple illustrative example of a serial batch program that
you would typically run using the \pkg{np} package.
\begin{verbatim}
## This is the serial version of npudensml_npRmpi.R for comparison
## purposes (bandwidth ought to be identical, timing may
## differ). Study the differences between this file and its MPI
## counterpart for insight about your own problems.

library(np)
options(np.messages=FALSE)

## Generate some data

n <- 2500

set.seed(42)

x <- rnorm(n)

## A simple example with likelihood cross-validation

t <- system.time(bw <- npudensbw(~x,
                                 bwmethod="cv.ml"))

summary(bw)

cat("Elapsed time =", t[3], "\n")
\end{verbatim}

Below is the same code modified to run in parallel using the
\pkg{npRmpi} package. The salient differences are as follows:

\begin{enumerate}
  
  \item You {\em must} copy the \code{Rprofile} file from the npRmpi/inst
    directory of the tarball/zip file into either your root directory
    or current working directory and rename it \code{.Rprofile}. 
    
  \item You will notice that there are some \code{mpi.foo} commands
    where \code{foo} is, for example, \code{bcast.cmd}. These are the
    \pkg{Rmpi} commands for telling the slave nodes what to run.
      
      The first thing we do is initialize the master and slave nodes
      using the \code{np.mpi.initialize()} command.
      
    \item Next we broadcast our data to the slave nodes using the
      \code{mpi.bcast.Robj2slave()} command which sends an
      \proglang{R} object to the slaves.
      
    \item After this, we might compute the data-driven
      bandwidths. Note we have wrapped the \pkg{np} command
      \code{npudensbw()} in the \code{mpi.bcast.cmd()} with the option
      \code{caller.execute=TRUE} which indicates it is to execute on
      the master and slave nodes simultaneously.
      
    \item Finally, we clean up gracefully by broadcasting the
      \code{mpi.quit()} command.
      
    \item There are a number of example files (including that above
      and below) in the \code{npRmpi/demo} directory that you may wish
      to examine. Each of these runs and has been deployed in a range
      of environments (Mac OS X, Linux).
        
\end{enumerate}


\begin{verbatim}
## Make sure you have the .Rprofile file from npRmpi/inst/ in your
## current directory or home directory. It is necessary.

## To run this on systems with OPENMPI installed and working, try
## mpirun -np 2 R CMD BATCH npudensml_npRmpi. Check the time in the
## output file foo.Rout (the name of this file with extension .Rout),
## then try with, say, 4 processors and compare run time.

## Initialize master and slaves.

mpi.bcast.cmd(np.mpi.initialize(),
              caller.execute=TRUE)

## Turn off progress i/o as this clutters the output file (if you want
## to see search progress you can comment out this command)

mpi.bcast.cmd(options(np.messages=FALSE),
              caller.execute=TRUE)

## Generate some data and broadcast it to all slaves (it will be known
## to the master node)

n <- 2500

mpi.bcast.cmd(set.seed(42),
              caller.execute=TRUE)

x <- rnorm(n)
mpi.bcast.Robj2slave(x)

## A simple example with likelihood cross-validation

t <- system.time(mpi.bcast.cmd(bw <- npudensbw(~x,
                                               bwmethod="cv.ml"),
                               caller.execute=TRUE))

summary(bw)

cat("Elapsed time =", t[3], "\n")

## Clean up properly then quit()

mpi.bcast.cmd(mpi.quit(),
              caller.execute=TRUE)
\end{verbatim}

For more examples including regression, conditional density
estimation, and semiparametric models, see the files in the
\code{npRmpi/demo} directory.  Kindly study these files and the
comments in each in order to extend the parallel examples to your
specific problem.

Note that the output from the serial and parallel runs ought to be
identical save for execution time. If they are not there is a problem
with the underlying code and I would ask you to kindly report such
things to me immediately along with the offending code.

\section{Summary}

The \pkg{npRmpi} package is a parallel implementation of the \pkg{np}
package that can exploit the presence of multiple processors and the
\proglang{MPI} interface for parallel computing to reduce the
computational run time associated with kernel methods. Run time is
inversely proportional to the number of available processors, so two
processors will complete a job in roughly one half the time of one
processor, ten in one tenth and so forth.\footnote{There is minor
  overhead involved with message passing, and for small samples the
  overhead can be substantial as the ratio of message passing to
  computing the kernel estimator increases - this will be negligible
  for sufficiently large samples.} Though installation of a working
\proglang{MPI} implementation requires some familiarity with computer
systems, local expertise exists for many and help is to be found
there. That being said, the Mac OS X operating system comes stock with
a fully functioning version of Open \proglang{MPI} so there is zero
additional effort required for the user in order to get up and running
in this environment. Finally, any feedback for improvements for this
document, reporting of errors and bugs and so forth is always
encouraged and much appreciated.

\bibliography{npRmpi}

\appendix

\section{Illustrative Timed Runs}\label{sec run time}

The times reported in Table \ref{run table} were generated using R
2.11-0 and Open MPI 1.2.8 on a 2008 vintage MacBook running Snow
Leopard 10.6.3 on a 2.4 GHz Intel Core 2 Duo (a completely stock
installation). Code was first run in serial mode using the np package
version 0.30-9 then in parallel mode with 2 processors using the
npRmpi package version 0.30-9. Elapsed time for the np functions is
provided (seconds) as is the ratio of the elapsed time for the
parallel run to the serial run.

Note that many of these illustrative examples use smallish sample
sizes hence the run time with 2 processors will not be 1/2 that with 1
processor due to overhead. But for larger samples (i.e. the ones you
actually need parallel computing for, not these toy illustrations) you
ought to see an improvement that is inversely related to the number of
processors.

\begin{table}
\begin{center}
  \caption{\label{run table}Illustrative timed runs (seconds) with 1
    processor (serial, np package) and 2 processors (parallel, npRmpi
    package).}
\begin{tabular}{lrrr}
Function & Secs(1) & Secs(2) & Ratio\cr
\hline
npcdens (ls) & 126.3 & 112.3 & 0.89\cr
npcdens (ml) & 50.1 & 25.5 & 0.51\cr
npcdistccdf & 68.0 & 35.7 & 0.53\cr
npcmstest & 73.9 & 51.9 & 0.70\cr
npconmode & 90.8 & 37.2 & 0.41\cr
npdeneqtest & 43.2 & 23.9 & 0.55\cr
npdeptest & 69.0 & 37.9 & 0.55\cr
npindex (Ichimura) & 36.7 & 19.9 & 0.54\cr
npindex (Klein/Spady) & 50.2 & 27.7 & 0.55\cr
npqreg & 95.9 & 51.7 & 0.54\cr
npreg (lc, aic) & 77.8 & 57.3 & 0.74\cr
npreg (lc, ls) & 81.8 & 49.7 & 0.61\cr
npreg (ll, aic) & 86.0 & 43.7 & 0.51\cr
npreg (ll, ls) & 89.3 & 46.0 & 0.52\cr
npscoef & 38.1 & 31.8 & 0.84\cr
npsdeptest & 79.4 & 58.6 & 0.74\cr
npsigtest & 143.2 & 87.3 & 0.61\cr
npsymtest & 43.2 & 30.5 & 0.71\cr
npudens (ls) & 60.9 & 30.6 & 0.50\cr
npudens (ml) & 25.3 & 14.5 & 0.57\cr
npunitest & 41.6 & 28.1 & 0.67\cr
\hline
\end{tabular}
\end{center}
\end{table}


\section{Known Issues}

\begin{enumerate}
  
\item It would be wise to cast all variables when read into
  \proglang{R} (always good practice) and not do so using the formula
  interface. 
  
  Casting responses (i.e.~stuff to the left of the \code{~}) works in
  the serial version but does not appear to work for \code{npcdensbw}.
  
\item The functions \code{npdeptest}, \code{npdeptest},
  \code{npsymtest} and \code{npunitest} currently run orders of
  magnitude slower under \proglang{MPI} when using
  \code{method="integration"} (default). This might be related to a
  limit on the number of nested function calls in \proglang{R} and/or
  the \pkg{cubature} package as I have encountered this in a serial
  environment in the past (solutions welcomed!).
  
  For the time being you are advised to use the serial \pkg{np}
  package if you require this function or select
  \code{method="summation"} (see \code{?npdeptest}, \code{?npdeptest},
  \code{?npsymtest} and \code{?npunitest} for caveats, though these
  functions are quite efficient computationally speaking and can
  handle quite large data sets by default via the serial
  implementation).
  
\item The \code{C} code underlying regression cross-validation
  currently differs between \pkg{np} and \pkg{npRmpi}. Both are
  correct with the latter being a tad slower (on one processor that is
  - see the timings above for a comparison). Note that, due to the
  different numerical implementations, each may deliver slightly
  different bandwidths. I hope to enlist the assistance of my
  co-creator on this package in the near future so that the same code
  underlies both packages throughout.
  
\end{enumerate}

\end{document}
